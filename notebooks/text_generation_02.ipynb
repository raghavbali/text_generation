{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Text Generation \n",
    "> A Practioners Guide : Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Generator from Scratch\n",
    "\n",
    "<img src=\"illustrations/tf_logo.png\" >\n",
    "\n",
    "We discussed about **RNNs** and **language models** in the previous notebook. Lets get our hands dirty and train our very own language model from scratch.\n",
    "\n",
    "We will train a language model using Tensorflow 2.0. TF2.0 is the updated version of the already popular deep learning framework. TF2.0 provides keras based high level APIs along with core set of functionality along with eager execution for more complex workflows. We will be relying this session using TF+Keras setup which is easy to understand and deploy.\n",
    "\n",
    "\n",
    "This notebook will leverage **GRUs** inplace of vanilla RNNs for two main reasons, better at handling vanishing and exploding gradients as well as ability to handle longer context. As far as corpus for training our language model, we utilize the famous book _**The Adventures of Sherlock Holmes**_ by _Sir Arthur Conan Doyle_. The book is made available through _Project Gutenberg_, check references section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version=2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version={}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = r'data/the_adventures_of_sherlock_holmes_1661-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book contains a total of 594197 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "text = open(datafile_path, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Book contains a total of {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick snippet of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. A SCANDAL IN BOHEMIA\r\n",
      "\r\n",
      "\r\n",
      "I.\r\n",
      "\r\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\r\n",
      "mention her under any other name. In his eyes she eclipses and\r\n",
      "predominates the whole of her \n"
     ]
    }
   ],
   "source": [
    "print(text[1300:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Text\n",
    "\n",
    "We shall perform bare minimum clean up of the text. The aim is to help our model understand the usage of words and its context. Typical preprocessing steps such as stopword removal, stemming, lower casing etc. are not required in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove first 1300 characters to remove \n",
    "# details related to project gutenberg\n",
    "text = text [1300:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Character Count | Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character to Integer Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '\"' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  ',' :  12,\n",
      "  '-' :  13,\n",
      "  '.' :  14,\n",
      "  '/' :  15,\n",
      "  '0' :  16,\n",
      "  '1' :  17,\n",
      "  '2' :  18,\n",
      "  '3' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Integer Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Sherlock Holmes, by ' ---- char-2-int ----  [61 74 68 71 59 67  2 37 71 68 69 61 75  2 75 64 61  2 65 75]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- char-2-int ----  {}'.format(repr(text[40:60]), text_as_int[40:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We leverage a sliding window approach to train out model. We first set the maximum sequence length to 100 characters. This is done for the purposes of preparing and training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      ".\n",
      " \n",
      "A\n",
      " \n",
      "S\n",
      "C\n",
      "A\n",
      "N\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard '\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'him\\r\\nmention her under any other name. In his eyes she eclipses and\\r\\npredominates the whole of her se'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'x. It was not that he felt any emotion\\r\\nakin to love for Irene Adler. All emotions, and that one part'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'icularly,\\r\\nwere abhorrent to his cold, precise but admirably balanced mind. He\\r\\nwas, I take it, the m'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'ost perfect reasoning and observing machine that\\r\\nthe world has seen, but as a lover he would have pl'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'aced himself in a\\r\\nfalse position. He never spoke of the softer passions, save with a gibe\\r\\nand a sne'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'er. They were admirable things for the observerâ€”excellent for\\r\\ndrawing the veil from menâ€™s motives an'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'d actions. But for the trained\\r\\nreasoner to admit such intrusions into his own delicate and finely\\r\\na'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'djusted temperament was to introduce a distracting factor which might\\r\\nthrow a doubt upon all his men'\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "'tal results. Grit in a sensitive\\r\\ninstrument, or a crack in one of his own high-power lenses, would n'\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Utility which takes a chunk of input text and target as one position shifted form of input chunk.\n",
    "    Parameters:\n",
    "        chunk: input list of words\n",
    "    Returns:\n",
    "        Tuple-> input_text(i.e. chunk minus last word),target_text(input chunk minus the first word)\n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'I. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard'\n",
      "Target data: '. A SCANDAL IN BOHEMIA\\r\\n\\r\\n\\r\\nI.\\r\\n\\r\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 38 ('I')\n",
      "  expected output: 14 ('.')\n",
      "Step    1\n",
      "  input: 14 ('.')\n",
      "  expected output: 2 (' ')\n",
      "Step    2\n",
      "  input: 2 (' ')\n",
      "  expected output: 30 ('A')\n",
      "Step    3\n",
      "  input: 30 ('A')\n",
      "  expected output: 2 (' ')\n",
      "Step    4\n",
      "  input: 2 (' ')\n",
      "  expected output: 48 ('S')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape=<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Dataset Shape={}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "\n",
    "We prepare a utility function to generate the architecture of our deep learning based language model. We leverage the high level ```tf.keras``` API for creating this model. We use only 1 hidden layer. You may experiment with additional layers as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \"\"\"\n",
    "    Utility to create a model object.\n",
    "    Parameters:\n",
    "        vocab_size: number of unique characters\n",
    "        embedding_dim: size of embedding vector. This typically in powers of 2, i.e. 64, 128, 256 and so on\n",
    "        rnn_units: number of GRU units to be used\n",
    "        batch_size: batch size for training the model\n",
    "    Returns:\n",
    "        tf.keras model object\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           24576     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 96)            98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Callbacks\n",
    "- We setup a single callback to store training checkpoints. \n",
    "- You may leverage other callbacks such as tensorboard, earlystopping etc as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = r'data/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train the ~DragonðŸ‰~ Language Model \n",
    "\n",
    "Now that we have prepared our training dataset along with our model, let us train it. We train it for a few epochs and observe the loss to understand whether it is learning or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "91/91 [==============================] - 337s 4s/step - loss: 2.9753\n",
      "Epoch 2/12\n",
      "91/91 [==============================] - 381s 4s/step - loss: 2.2011\n",
      "Epoch 3/12\n",
      "91/91 [==============================] - 498s 5s/step - loss: 1.9793\n",
      "Epoch 4/12\n",
      "91/91 [==============================] - 441s 5s/step - loss: 1.7988\n",
      "Epoch 5/12\n",
      "91/91 [==============================] - 437s 5s/step - loss: 1.6527\n",
      "Epoch 6/12\n",
      "91/91 [==============================] - 387s 4s/step - loss: 1.5329\n",
      "Epoch 7/12\n",
      "91/91 [==============================] - 413s 5s/step - loss: 1.4404\n",
      "Epoch 8/12\n",
      "91/91 [==============================] - 400s 4s/step - loss: 1.3671\n",
      "Epoch 9/12\n",
      "91/91 [==============================] - 393s 4s/step - loss: 1.3109\n",
      "Epoch 10/12\n",
      "91/91 [==============================] - 392s 4s/step - loss: 1.2607\n",
      "Epoch 11/12\n",
      "91/91 [==============================] - 394s 4s/step - loss: 1.2185\n",
      "Epoch 12/12\n",
      "91/91 [==============================] - 395s 4s/step - loss: 1.1772\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Text\n",
    "\n",
    "We trained out model on the text from _The Adventures of Sherlock Holmes_. Now one should notice that we literally did not perform any preprocessing on the text apart from removing some metadata and table of contents. The model is trained with a vocab size of **96** unique characters which includes numbers and special characters apart from lower and upper case letters.\n",
    "\n",
    "We should also note that we have trained a character level language model to reduce the vocab size. Imagine the vocab size for training at the word level, wouldn't it be orders of magnitude larger than this? Also imagine the amount of training data required to help the model understand different contexts under which a specific word might be used.\n",
    "\n",
    "Let us generate some text and see what our model has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/training_checkpoints/ckpt_12'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the latest checkpoint from the model directory\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load\n",
    "> Notice that we trained the model with certain batch size. Using ```model.summary``` we saw how the batch size shows up as one the parameters which determine input's shape.\n",
    "\n",
    "> For inference, we would be using a single input sentence/context to generate text. Thus we build the model again using ```build_model``` utility we prepared earlier but use a ```batch_size``` of 1 this time. Once we have the model object with desired batch size, we use ```load_weights``` to utilize the latest checkpoint weights for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            24576     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 96)             98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_string, num_generate=1000,temperature=1.0):\n",
    "    \"\"\"\n",
    "    Utility to generate text given a trained model and context\n",
    "    Parameters:\n",
    "        model: tf.keras object trained on a sufficiently sized corpus\n",
    "        context_string: input string which acts as context for the model\n",
    "        num_generate: number of characters to be generated\n",
    "        temperature: parameter to control randomness of outputs\n",
    "    Returns:\n",
    "        string : context_string+text_generated\n",
    "    \"\"\"\n",
    "\n",
    "    # vectorizing: convert context string into string indices\n",
    "    input_eval = [char2idx[s] for s in context_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String for generated characters\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    # Loop till required number of characters are generated\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # temperature helps control the character returned by the model.\n",
    "        predictions = predictions / temperature\n",
    "        # Sampling over a categorical distribution\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # predicted character acts as input for next step\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (context_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Temperature** is a scaling factor. This parameter helps in controlling the output randomness of our model. Lower values of temperature helps generate predictable outputs which higher values are difficult to understand. Higher temperatures may lead to unseen or non-dictionary words while the chances are lesser if the temperature is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us generate some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are certainly leave\r\n",
      "your husbandâ€™s read, and then two vareak illsted of bory, it was young McCarthyâ€™t \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are that prsilU\r",
      "S4Å“lder, he\r\n",
      "int always,\r\n",
      "heabveirnies brokeve at TogheatudeprÃ©gail I; â€˜Ychâ€”_ OR2UÃ ,Dâ€œW\n"
     ]
    }
   ],
   "source": [
    "# We increase the temperature, i.e. increase randomness\n",
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100,temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson you are the news\r\n",
      "all open. It was a man who was a man with a sudden brightly more than I cannot means to w\n"
     ]
    }
   ],
   "source": [
    "# We decrease the temperature, i.e. increase randomness\n",
    "print(generate_text(model, context_string=u\"Watson you are\",num_generate=100,temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained to handle input integer sequences to generate next integer as output. We use ```char2int``` and ```int2char``` mapping objects to convert characters to integer sequences for input and integer to character respectively. The ```generate_text``` utility we used above used every output prediction as input for the next time step (ignore temperature for now). This method of using the highest probability prediction as output is called **Greedy Decoding**. Greeding decoding is fast and simple but is marred with issues we saw in samples we just generated.\n",
    "\n",
    "Focusing on only highest probability output narrows our model's focus to just the next step which inturn may result in inconsistent or non-dictionary terms/words.\n",
    "\n",
    "There are a few other decoding strategies developed over the years. Let us understand them at a high level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoding simply picks up the highest probability character or word at every timestep. We saw in the examples above the issues associated with such a simple approach.\n",
    "\n",
    "Beam search is the obvious next step to improve the output predictions from the model. Instead of being greedy, beam search keeps track of _n_ paths at any given time and selects the path with overall higher probability. Let us understand this better through an example. The following is output from a text generation model with beam search size of **2**:\n",
    "<img src=\"illustrations/beamsearch.png\" width=\"600\">\n",
    "\n",
    "- **At time step t<sub>1</sub>**:\n",
    "    - Top three outputs (with probabilities) are, **certainly(0.6), the(0.3) and in(0.1)**\n",
    "    - In case of beam search, we would have selected certainly as our output since it has the highest probability of 0.6.\n",
    "    - In this case, with beam size of 2, we keep track of top 2 words, i.e. **certainly(0.6) and the(0.3)**\n",
    "    \n",
    "- **At time step t<sub>2</sub>**:\n",
    "    - We again repeat the same activity, i.e. keep a track of top two terms from each of the two beams\n",
    "    - The beams are calculated as:\n",
    "        + certainly(0.6) -> smart(0.4) = 0.6*0.4 = 0.24\n",
    "        + the(0.3) -> man(0.9) = 0.3*0.9 = 0.27\n",
    "        \n",
    "Finally the model selects \n",
    ">_Watson you are_ **the man** \n",
    "\n",
    "This beam had a final probability of **0.27** as compared to **certainly smart** which ended up with 0.24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling refers to randomly picking a word from a probability distribution. In the case of language generation, sampling helps us select the next word conditioned on the context so far. \n",
    "**$$w_t = P(w|w_{1:t-1})$$**\n",
    "\n",
    "Continuing from the previous example:\n",
    "<img src=\"illustrations/sampling.png\" width=\"600\">\n",
    "This method picks a random word at every timestep from the given conditional probability. In case of our example, the model ended by randomly selecting **in** and then **trouble** as subsequent outputs. If you notice carefully, at timestep t<sub>1</sub> the model ends up selecting the word with least probability.\n",
    "\n",
    "This brings in much required randomness (yet be coherent) as it is associated with a way we humans use language. [Holtzman et. al](https://arxiv.org/abs/1904.09751) in their work present this exact argument by stating that humans do not simply use the words with highest probability only with language. The following is an observation from their work:\n",
    "<img src=\"illustrations/holtzman.png\" width=\"600\">\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### Temperature\n",
    "Though sampling helps bring in required amount of randomness, it is not free from issues. Random sampling leads to gibberish and incoherence at times. To control the amount of randomness, we introduce ```temperature```. This parameter helps increase the likelihood of high probability terms reduce the likelihood of low probability ones. This leads to sharper distributions. High temperature leads to more randomness while lower temperature brings in predictability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is a combination of **beam search and sampling**. In simple terms, at every timestep, instead of selecting a random word we keep a track of **top k terms**(similar to beam search) and redistribute the probabilities amongst them. This helps the model have additional scope of generating coherent samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to top-k sampling but differs in the way we select the k. Instead of deciding upon a fixed scope (i.e. k), we set a threshold probability ```p```. This threshold helps the model dynamically select its scope at every timestep. Assume we select $$p=0.92$$. At timestep t<sub>1</sub> it could be the case that it takes 5 words to cross the threshold while at timestep t<sub>2</sub> it might shrink to just 3. This ability to expand and shrink the scope helps generate even more human like output samples.\n",
    "\n",
    "For more details, please refer to the works of [Holtzman et. al](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ [Project Gutenberg :The Adventures of Sherlock Holmes](https://www.gutenberg.org/ebooks/1661)\n",
    "+ [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "+ [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
