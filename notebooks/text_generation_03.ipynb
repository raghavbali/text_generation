{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Text Generation \n",
    "> A Practioners Guide : Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Moment for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its been quite sometime since we were introduced to the amazing representation prowess of methods such as **word2vec** and **GloVe**. Since then rapid improvements in representational capacity and concepts such as **transfer learning** have resulted in tremendous improvements in the field of Natural Language Processing. Sebastian Ruder in his famous article \"[NLP's ImageNet moment has arrived](https://ruder.io/nlp-imagenet/)\" pointed out the turning point in 2018.\n",
    "\n",
    "<img src=\"illustrations/imagenet_challenge.png\" width=\"600\">\n",
    "\n",
    "[Source](https://www.slideshare.net/xavigiro/image-classification-on-imagenet-d1l4-2017-upc-deep-learning-for-computer-vision)\n",
    "\n",
    "We have seen successive improvements in the form enormously powerful architectures like BERT, GPT2, XLNET and the likes since then. These models take a unique approach in modeling natural language to present state of the art results in most downstream tasks. But what lead to this sudden shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is All you Need ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leveraged a basic RNN based network to generate text in the previous notebook. To enhance performance of **sequence to sequence** tasks a typical **Encoder-Decoder** architecture is the go-to choice.\n",
    "\n",
    "<img src=\"illustrations/encoder_decoder.png\">\n",
    "\n",
    "Let us consider the case of Machine Translation, i.e. translation of English to Spanish (or any other language). In a typical **Encoder-Decoder** architecture, the _Encoder_ takes in the input text in English as input and prepares a condensed vector representation of the whole input. Typically termed as _bottleneck_ features. The _Decoder_ then uses these features to generate the translated text in Spanish.\n",
    "\n",
    "While this architecture and its variants worked wonders, they had issues. Issues such as inability handle longer input sequences, cases where there is not a one to one mapping between input vs output language and so on. \n",
    "\n",
    "\n",
    "To handle these issues, Vasvani et. al. in their now famouly titled paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) build up on the concepts of attention. Attention was introduced in the works of [Bahdanau et. al.](https://arxiv.org/abs/1409.0473) to handle the task of machine translation. The main highlight of this work was the **Transformer** architecture. **Transformers** were shown to present state of the art results on multiple benchmarks without using any recurrence or convolutional components.\n",
    "<img src=\"illustrations/transformer.png\" width=\"400\">\n",
    "\n",
    "\n",
    "The concept of **Attention** is a simple yet important one. In layman terms, it helps the model focus on not just the current input but also determine specific pieces of information from the past. This helps in models which are able to handle long range dependencies along with scenarios where there is not a one to one mapping between inputs and outputs. The following is a sample illustration from the paper demonstrating the focus/attention of the model on the words when _making_ is the input.\n",
    "\n",
    "<img src=\"illustrations/attention.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture presented by [Vasvani et al](https://arxiv.org/abs/1706.03762) was just the begining. With recurrent components out of scope, researchers explored more complex architectures. The following figure shows different architectures and their number of parameters:\n",
    "<img src=\"illustrations/nlp_models.png\">\n",
    "[Source](https://miro.medium.com/max/2070/1*IFVX74cEe8U5D1GveL1uZA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On a mission to solve NLP,\n",
    "one commit at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As their tagline explains, they are helping solve NLP problems. While the transformer revolution changed things for language related tasks, using them was not a simple thing. With number of parameters running into billions, these models were out of reach for most researchers and application developers.\n",
    "\n",
    "Hugging Face changed the scene by developing the ```transformers``` package. The ```transformers``` package is a one stop shop for all your transformer type architectures. This package provides standard interfaces to handle tasks such as ```tokenization```, ```decoding```, ```fine tuning```, ```vectorization``` and so on. The package supports ```pytorch``` and ```tensorflow``` backends for ease of use.\n",
    "\n",
    "\n",
    "Let us get started with our quick handson session with Hugging Face ```transformers```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.9.1-py3-none-any.whl (641 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 641 kB 543 kB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 153 kB 543 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.5.14.tar.gz (696 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 696 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.90-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 360 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 352 kB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñä                           | 174 kB 245 kB/s eta 0:00:05\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 883 kB 612 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: requests in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: six in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/swatigaur/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Building wheels for collected packages: regex, sacremoses\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.5.14-cp37-cp37m-macosx_10_9_x86_64.whl size=285961 sha256=740f39cea7f8208cbb6fc288d64c4d6c3fb8873acd286b5bb645b02f285a1987\n",
      "  Stored in directory: /Users/swatigaur/Library/Caches/pip/wheels/d4/9f/f1/da2c5e041f3f291659e2aca93cbe48355347b3e9e7c836ec08\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=27a1ab11fb14ceedb430ae4a23bbb93461561e84c70f328d25a93ef18828ed06\n",
      "  Stored in directory: /Users/swatigaur/Library/Caches/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built regex sacremoses\n",
      "Installing collected packages: regex, sentencepiece, tokenizers, sacremoses, transformers\n",
      "Successfully installed regex-2020.5.14 sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "from numpy import random\n",
    "from transformers import (TFGPT2LMHeadModel,\n",
    "                          GPT2Tokenizer,\n",
    "                          GPT2Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version=2.0.0\n",
      "huggingface/transformer version=2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(\"tf version={}\".format(tf.__version__))\n",
    "print(\"huggingface/transformer version={}\".format(transformers.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c20e359404efcaf4c30a5a4dfaf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=718.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fd253e92ec47aea9951032cdcf0146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc2d4047f0e4df6bd812a311f1ff460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcaf803148a46e5a92baf41785d04c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1419628976.0, style=ProgressStyle(descr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=392285, shape=(1, 4), dtype=int32, numpy=array([[   54, 13506,   345,   389]], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Watson you are', return_tensors='tf')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Watson you are a great guy and I hope you are doing well. I am sorry to hear\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=20)\n",
    "\n",
    "print(\"Output:\\n\" + 110 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampled Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "0: Watson you are in an accident?\n",
      "\n",
      "Pam: Oh yes, and I would tell\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1: Watson you are in a great spot if the team wants to sign your kid. He will make\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "2: Watson you are no longer allowed to leave your job but your job does not matter. He tells\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# Use a combination of decoding techniques\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=20, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 110 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    print(\"-\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
